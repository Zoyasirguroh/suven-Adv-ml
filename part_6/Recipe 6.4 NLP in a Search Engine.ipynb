{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recipe 6-4 : NLP in a Search Engine (discussion Case-Study)\n",
    "--\n",
    "In this recipe, we are going to discuss what it takes to build a search engine\n",
    "from an NLP standpoint. Implementation of the same is beyond the scope\n",
    "of this course.\n",
    "\n",
    "Problem\n",
    "--\n",
    "You want to know the architecture and NLP pipeline to build a search engine.\n",
    "\n",
    "Solution\n",
    "--\n",
    "Figure below shows the whole process. Each step is explained in the “How It\n",
    "Works” section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NLP_in_searchEngines](images/NLP_in_searchEngines.png 'NLP_in_searchEngines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEO -> I will make my website Google friendly, so that Google SE can see me fast.  \n",
    "\n",
    "\n",
    "Ranking -> Quality Score(#clicks_on_url, retention time, bounce_rate, .....) \n",
    "\n",
    "SEO helps in improving the Quality Score.\n",
    "\n",
    "SO, if i do SEO , my site QS will improve and in turn my \n",
    "site's ranking will improve, which will be respected by our ML model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How It Works\n",
    "--\n",
    "Let’s follow and understand the above architecture step by step in this\n",
    "section to build the search engine from an NLP standpoint.\n",
    "\n",
    "> Step 4-1 Preprocessing : \n",
    "Whenever the user enters the search query, it is passed on to the NLP\n",
    "preprocessing pipeline:\n",
    "\n",
    "1. Removal of noise and stop words\n",
    "2. Tokenization\n",
    "3. Stemming\n",
    "4. Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Step 6-2 The entity extraction model\n",
    "Output from the above pipeline is fed into the entity extraction model.\n",
    "\n",
    "We can build the customized entity recognition model by using any of the\n",
    "libraries like StanfordNER or NLTK. Or you can build an entity recognition model from scratch using conditional random fields or Markov models."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For example, suppose we are building a search engine for an\n",
    "e-commerce giant. Below are entities that we can train the model on:\n",
    "    \n",
    "• Gender\n",
    "• Color\n",
    "• Brand\n",
    "• Product Category\n",
    "• Product Type\n",
    "• Price\n",
    "• Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we can build named entity disambiguation using deep learning frameworks like RNN and LSTM. This is very important for the entities extractor to understand the content in which the entities are used. For example, pink can be a color or a brand. NED helps in such disambiguation.\n",
    "\n",
    "> NERD Model building steps:\n",
    "\n",
    "> 1. Data cleaning and preprocessing\n",
    "\n",
    "> 2. Training NER Model\n",
    "\n",
    "> 3. Testing and Validation\n",
    "\n",
    "> 4. Deployment\n",
    "\n",
    "\n",
    "> Ways to train/build NERD model:\n",
    "\n",
    "> 1. Named Entity Recognition and Disambiguation\n",
    "\n",
    "> 2. Stanford NER with customization\n",
    "\n",
    "> 3. Recurrent Neural Network (RNN) – LSTM (Long Short-Term Memory) to use context for disambiguation\n",
    "\n",
    "> 4. Joint Named Entity Recognition and Disambiguation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4-3 Query enhancement/expansion\n",
    "--\n",
    "It is very important to understand the possible synonyms of the entities to\n",
    "make sure search results do not miss out on potential relevance. Say, for\n",
    "example, men’s shoes can also be called as male shoes, men’s sports shoes,\n",
    "men’s formal shoes, men’s loafers, men’s sneakers.\n",
    "\n",
    "Use locally-trained word embedding (using Word2Vec / GloVe Model ) to achieve this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4-4  Use a search platform\n",
    "--\n",
    "Search platforms such as Solr or Elastic Search have major features that\n",
    "include full-text search hit highlighting, faceted search, real-time indexing,\n",
    "dynamic clustering, and database integration. \n",
    "\n",
    "This is not related to NLP; but as an end-to-end application point of view, we have just given an introduction of what this is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4-5 Learning to rank\n",
    "--\n",
    "Once the search results are fetched from Solr or Elastic Search, they should\n",
    "be ranked based on the user preferences using the past behaviors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
