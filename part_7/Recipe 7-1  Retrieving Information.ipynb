{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning for NLP\n",
    "--\n",
    "\n",
    "Recipe 7-1:  Retrieving Information\n",
    "--\n",
    "\n",
    "Information retrieval is one of the highly used applications of NLP and it is\n",
    "quite tricky. The meaning of the words or sentences not only depends on the exact words used but also on the context and meaning. Two sentences may be of completely different words but can convey the same meaning. We should be able to capture that as well.\n",
    "\n",
    "An information retrieval (IR) system allows users to efficiently search documents and retrieve meaningful information based on a search text/query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Information_retrieval_block_dg](images/Information_retrieval_block_dg.png  'Information_retrieval_block_dg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem\n",
    "--\n",
    "Information retrieval using word embeddings.\n",
    "\n",
    "Solution\n",
    "--\n",
    "There are multiple ways to do Information retrieval. But we will see how to\n",
    "do it using word embeddings, which is very effective since it takes context\n",
    "also into consideration. \n",
    "\n",
    "> We discussed how word embeddings are built in Chapter 4. \n",
    "\n",
    "We will just use the pretrained word2vec in this case. \n",
    "\n",
    "Let’s take a simple example and see how to build a document retrieval using query input. Let’s say we have 4 documents in our database as below. (Just showcasing how it works. We will have too many documents in a real-world application.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc1 = [\"With the Union cabinet approving the amendments to the Motor Vehicles Act, 2016, those caught for drunken driving will have to have really deep pockets, as the fine payable in court has been enhanced to Rs 10,000 for first-time offenders.\" ]\n",
    "        \n",
    "Doc2 = [\"Natural language processing (NLP) is an area of computer science and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.\"]\n",
    "\n",
    "Doc3 = [\"He points out that public transport is very good in Mumbai and New Delhi, where there is a good network of suburban and metro rail systems.\"] \n",
    "\n",
    "Doc4 = [\"But the man behind the wickets at the other end was watching just as keenly. With an affirmative nod from Dhoni, India captain Rohit Sharma promptly asked for a review. Sure enough, the ball would have clipped the top of middle and leg.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we have numerous documents like this. And you want to retrieve\n",
    "the most relevant one, for the query “cricket.” Let’s see how to build it.\n",
    "\n",
    "query = \"cricket\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How It Works\n",
    "\n",
    "# Step 7.1.1 : Import the libraries\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import nltk\n",
    "import itertools\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import scipy\n",
    "from scipy import spatial\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7.1.2 - Create/import documents\n",
    "\n",
    "# Doc1 , Doc2 , Doc3 and Doc4 as defined above in the code.\n",
    "\n",
    "# Put all the documents in one list\n",
    "fin = Doc1+Doc2+Doc3+Doc4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7.1.3 Download word2vec\n",
    "\n",
    "As mentioned earlier, we are going to use the word embeddings to solve\n",
    "this problem. We had downloaded word2Vec in Jupyter Notebook \"Converting Text to Features\". Also recall that we may encounter ValueError as below\n",
    "> The Google Db is soo large that we would get ValueError, like this : ValueError: array is too big; arr.size * arr.dtype.itemsize is larger than the maximum possible size.\n",
    "\n",
    "Download link for word2vec is:\n",
    "https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 7.1.3\n",
    "# load the model\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('C:\\Program Files\\Python36\\suven\\Adv ML\\datasets\\datasets/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes for 7.1 :     // case 1: \n",
    "// assume word = \"cricket\"\n",
    "\n",
    "if \"cricket\" in model.wv.vocab: -> true\n",
    "\n",
    "  return model[word]  -> word found hence vector \n",
    "  \n",
    "  returned\n",
    "  \n",
    "  \n",
    " else:\n",
    " \n",
    " \n",
    "  return np.zeros(300)\n",
    "  \n",
    "model \n",
    "-----\n",
    "  apple : [0.3,0.445,.......    ],size = 300 \n",
    "  bat : [0.34,0.245,.......    ],size = 300\n",
    "  cricket : [0.2,0.665,.......    ], size = 300\n",
    "  ...\n",
    "  ...\n",
    "--------------------\n",
    "// case 2: \n",
    "// assume word = \"hitesh\"\n",
    "\n",
    "if \"hitesh\" in model.wv.vocab: -> false\n",
    "  \n",
    "  \n",
    "  return model[word]  \n",
    " \n",
    " \n",
    " else:\n",
    "  \n",
    "  \n",
    "  return np.zeros(300) -> make a vector of 300 0's \n",
    " \n",
    " and return it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7.1.4 : Create IR system\n",
    "\n",
    "# Now we build the information retrieval system:\n",
    "\n",
    "# Preprocessing\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    " pattern = r'[^a-zA-z0-9\\s]'\n",
    " text = re.sub(pattern, \", \".join(text))\n",
    " tokens = tokenizer.tokenize(text)\n",
    " tokens = [token.strip() for token in tokens]\n",
    " if is_lower_case:\n",
    "  filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    " else:\n",
    "  filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "  filtered_text = ' '.join(filtered_tokens)\n",
    " return filtered_text\n",
    "\n",
    "\n",
    "# Function to get the embedding vector for n dimension, we have used \"300\"\n",
    "def get_embedding(word):\n",
    " if word in model.wv.vocab:\n",
    "  return model[x]\n",
    " else:\n",
    "  return np.zeros(300)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For every document, we will get a lot of vectors based on the number of\n",
    "words present. We need to calculate the average vector for the document\n",
    "through taking a mean of all the word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sub() missing 1 required positional argument: 'string'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-018606c5fa8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mout_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msen\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# this loop will pick one sentence at a time from fin (final document)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m  \u001b[0maverage_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mget_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremove_stopwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m  \u001b[0mdict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m \u001b[0msen\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0maverage_vector\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m  \u001b[0mout_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-bde033711b72>\u001b[0m in \u001b[0;36mremove_stopwords\u001b[1;34m(text, is_lower_case)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_lower_case\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m  \u001b[0mpattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr'[^a-zA-z0-9\\s]'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m  \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\", \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m  \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m  \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: sub() missing 1 required positional argument: 'string'"
     ]
    }
   ],
   "source": [
    "# Getting average vector for each document\n",
    "out_dict = {}\n",
    "for sen in fin:  # this loop will pick one sentence at a time from fin (final document)\n",
    " average_vector = (np.mean(np.array([get_embedding(x) for x in nltk.word_tokenize(remove_stopwords(sen))]), axis=0))\n",
    " dict = { sen : (average_vector) }\n",
    " out_dict.update(dict)\n",
    "\n",
    "# Function to calculate the similarity between the query vector and document vector\n",
    "def get_sim(query_embedding, average_vector_doc):\n",
    " sim = [(1 - scipy.spatial.distance.cosine(query_embedding, average_vector_doc))]\n",
    " return sim\n",
    "\n",
    "\n",
    "\n",
    "# Remember : scipy.spatial.distance.cosine gives us dissimilarity instead of similarity\n",
    "# therefore 1 - (....)\n",
    "# Note : scipy...cosine is different from the cosine distance formulae, we used in ML\n",
    "# Refer : https://datascience.stackexchange.com/questions/8426/cosine-distance-1-in-scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank all the documents based on the similarity to get relevant docs\n",
    "def Ranked_documents(query):\n",
    " query_words = (np.mean(np.array([get_embedding(x) for x in nltk.word_tokenize(query.lower())],dtype=float), axis=0))\n",
    " rank = []\n",
    " \n",
    " for k,v in out_dict.items():\n",
    "  rank.append((k, get_sim(query_words, v)))\n",
    " \n",
    " rank = sorted(rank,key=lambda t: t[1], reverse=True)\n",
    " print('Ranked Documents :')\n",
    " return rank"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Step 7.1.5 :  Results and applications\n",
    "Let’s see how the information retrieval system we built is working with a\n",
    "couple of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\ipykernel_launcher.py:21: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-5c9a46b8496c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Call the IR function with a query\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mRanked_documents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cricket\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-7e0dda996ad7>\u001b[0m in \u001b[0;36mRanked_documents\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Rank all the documents based on the similarity to get relevant docs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mRanked_documents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m  \u001b[0mquery_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mget_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m  \u001b[0mrank\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-7e0dda996ad7>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Rank all the documents based on the similarity to get relevant docs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mRanked_documents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m  \u001b[0mquery_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mget_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m  \u001b[0mrank\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-bde033711b72>\u001b[0m in \u001b[0;36mget_embedding\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m  \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m  \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "# Call the IR function with a query\n",
    "Ranked_documents(\"cricket\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On running this code , over a PC (with 16GB RAM and the Microsoft Visual Studio - C++ build tools correctly installed), our team member got the following o/p  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[('But the man behind the wickets at the other end was watching\n",
    "just as keenly. With an affirmative nod from Dhoni, India\n",
    "captain Rohit Sharma promptly asked for a review. Sure enough,\n",
    "the ball would have clipped the top of middle and leg.',\n",
    "[0.44954327116871795]),\n",
    "('He points out that public transport is very good in Mumbai\n",
    "and New Delhi, where there is a good network of suburban and\n",
    "metro rail systems.',\n",
    "[0.23973446569030055]),\n",
    "('With the Union cabinet approving the amendments to the Motor\n",
    "Vehicles Act, 2016, those caught for drunken driving will have\n",
    "to have really deep pockets, as the fine payable in court has\n",
    "been enhanced to Rs 10,000 for first-time offenders.',\n",
    "[0.18323712012013349]),\n",
    "('Natural language processing (NLP) is an area of computer\n",
    "science and artificial intelligence concerned with the\n",
    "interactions between computers and human (natural) languages,\n",
    "in particular how to program computers to process and analyze\n",
    "large amounts of natural language data.',\n",
    "[0.17995060855459855])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you see, doc4 (on top in result), this will be most relevant for the\n",
    "# query “cricket” even though the word “cricket” is not even mentioned once\n",
    "# with the similarity of 0.449.\n",
    "\n",
    "# Let’s take one more example as may be driving.\n",
    "Ranked_documents(\"driving\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[('With the Union cabinet approving the amendments to the Motor\n",
    "Vehicles Act, 2016, those caught for drunken driving will have\n",
    "to have really deep pockets, as the fine payable in court has\n",
    "been enhanced to Rs 10,000 for first-time offenders.',\n",
    "[0.35947287723800669]),\n",
    "('But the man behind the wickets at the other end was watching\n",
    "just as keenly. With an affirmative nod from Dhoni, India\n",
    "captain Rohit Sharma promptly asked for a review. Sure enough,\n",
    "the ball would have clipped the top of middle and leg.',\n",
    "[0.19042556935316801]),\n",
    "('He points out that public transport is very good in Mumbai\n",
    "and New Delhi, where there is a good network of suburban and\n",
    "metro rail systems.',\n",
    "[0.17066536985237601]),\n",
    "('Natural language processing (NLP) is an area of computer\n",
    "science and artificial intelligence concerned with the\n",
    "interactions between computers and human (natural) languages,\n",
    "in particular how to program computers to process and analyze\n",
    "large amounts of natural language data.',\n",
    "[0.088723080005327359])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, since driving is connected to transport and the Motor Vehicles\n",
    "# Act, it pulls out the most relevant documents on top. The first 2 documents\n",
    "# are relevant to the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the same approach and scale it up for as many documents as possible. \n",
    "\n",
    "For more accuracy, we can build our own embeddings, as we learned in Chapter 4, for specific industries since the one we are using is generalized.\n",
    "(Although it is time consuming task)\n",
    "\n",
    "This is the fundamental approach that can be used for many applications like the following:\n",
    "1. Search engines (Internet Search Engines)\n",
    "2. Document retrieval\n",
    "3. Passage retrieval\n",
    "4. Question and answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![result_length_vs_query_len](images/result_length_vs_query_len.png 'result_length_vs_query_len' )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "It’s been proven that results will be good when queries are longer and\n",
    "the result length is shorter. That’s the reason we don’t get great results in\n",
    "search engines when the search query has lesser number of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strongly Recommended :  I have learned a lot from the Videos of Mr. Brandon Rohrer. I would advice each of my student to go through this free course. You may skip the CNN part.  https://end-to-end-machine-learning.teachable.com/p/how-deep-neural-networks-work"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
