{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text classification & Sentiment analysis\n",
    "--\n",
    "\n",
    "Text classification – The aim of text classification is to automatically classify the text documents based on pretrained categories.\n",
    "\n",
    "Applications:\n",
    "--\n",
    "1. Sentiment Analysis\n",
    "2. Document classification\n",
    "3. Spam – ham mail classification\n",
    "4. Resume shortlisting\n",
    "5. Document summarization\n",
    "\n",
    "Problem\n",
    "--\n",
    "to do : Spam - ham classification using machine learning.\n",
    "\n",
    "Solution\n",
    "--\n",
    "If you observe, your Gmail has a folder called “Spam.” It will basically\n",
    "classify your emails into spam and ham so that you don’t have to read\n",
    "unnecessary emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['v1', 'v2', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], dtype='object')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let’s follow the step-by-step method to build the classifier.\n",
    "\n",
    "# Step 1 :  Data collection and understanding\n",
    "# Please download data from the below link and save it in your working directory:\n",
    "# https://www.kaggle.com/uciml/sms-spam-collection-dataset#spam.csv\n",
    "\n",
    "import pandas as pd\n",
    "#Read the data\n",
    "Email_Data = pd.read_csv(\"C:\\Program Files\\Python36\\suven\\Adv ML\\datasets\\datasets/spam.csv\",encoding ='latin1')\n",
    "\n",
    "#Data undestanding\n",
    "Email_Data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Target                                              Email\n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Email_Data = Email_Data[['v1', 'v2']]\n",
    "Email_Data = Email_Data.rename(columns={\"v1\":\"Target\",\"v2\":\"Email\"})\n",
    "Email_Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2 : Text processing and feature engineering\n",
    "\n",
    "# all imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import Word\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import sklearn.feature_extraction.text as text\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go jurong point, crazy.. avail bugi n great wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar... joke wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entri 2 wkli comp win fa cup final tkt 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say earli hor... u c alreadi say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah think goe usf, live around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Target                                              Email\n",
       "0    ham  go jurong point, crazy.. avail bugi n great wo...\n",
       "1    ham                        ok lar... joke wif u oni...\n",
       "2   spam  free entri 2 wkli comp win fa cup final tkt 21...\n",
       "3    ham          u dun say earli hor... u c alreadi say...\n",
       "4    ham              nah think goe usf, live around though"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pre processing steps like lower case, stemming and lemmatization\n",
    "Email_Data['Email'] = Email_Data['Email'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "stop = stopwords.words('english')\n",
    "Email_Data['Email'] = Email_Data['Email'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "st = PorterStemmer()\n",
    "\n",
    "Email_Data['Email'] = Email_Data['Email'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "Email_Data['Email'] = Email_Data['Email'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "Email_Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.47787054, 0.27951112, 0.37828744, ..., 0.35897604, 0.41781432,\n",
       "       0.5023328 ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 3:\n",
    "# Splitting data into train and validation\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(Email_Data['Email'], Email_Data['Target'])\n",
    "\n",
    "# TFIDF feature generation for a maximum of 5000 features\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "# token_pattern : string\n",
    "# Regular expression denoting what constitutes a “token”, \n",
    "# only used if analyzer == 'word'. The default regexp select tokens \n",
    "# of 2 or more alphanumeric characters \n",
    "# (punctuation is completely ignored and always treated as a token separator).\n",
    "# TFIDF feature generation for a maximum of 5000 features\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "\n",
    "tfidf_vect.fit(Email_Data['Email'])\n",
    "xtrain_tfidf = tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf = tfidf_vect.transform(valid_x)\n",
    "xtrain_tfidf.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3929)\t0.4778705383733631\n",
      "  (0, 2223)\t0.2795111230624288\n",
      "  (0, 1384)\t0.37828744113012275\n",
      "  (0, 1272)\t0.5376632661032151\n",
      "  (0, 593)\t0.5112045625829047\n",
      "  (1, 1963)\t0.8070812177243439\n",
      "  (1, 1760)\t0.5904404356042955\n",
      "  (2, 4901)\t0.39739368066143715\n",
      "  (2, 3139)\t0.4720545393636423\n",
      "  (2, 2554)\t0.4653531360327522\n",
      "  (2, 1632)\t0.6345779961683339\n",
      "  (3, 4988)\t0.38880812052149\n",
      "  (3, 4078)\t0.6099628001677578\n",
      "  (3, 3881)\t0.31334400620450503\n",
      "  (3, 1144)\t0.32183173418324934\n",
      "  (3, 602)\t0.5244172923124943\n",
      "  (4, 4914)\t0.6806103075861311\n",
      "  (4, 3007)\t0.7326456232091421\n",
      "  (5, 4731)\t0.11458785022397502\n",
      "  (5, 4320)\t0.18453401061303262\n",
      "  (5, 3599)\t0.9226700530651631\n",
      "  (5, 2223)\t0.09269584850499432\n",
      "  (5, 2184)\t0.12322059078287013\n",
      "  (5, 2125)\t0.14575863889528984\n",
      "  (5, 1237)\t0.15280108781705792\n",
      "  :\t:\n",
      "  (4176, 544)\t0.3130316493157341\n",
      "  (4177, 4977)\t0.30032710604240054\n",
      "  (4177, 3469)\t0.1971737897992139\n",
      "  (4177, 3210)\t0.2628910618972912\n",
      "  (4177, 3018)\t0.18767279619098437\n",
      "  (4177, 2833)\t0.17507795277237972\n",
      "  (4177, 1727)\t0.22639871381568688\n",
      "  (4177, 1585)\t0.27018990519073904\n",
      "  (4177, 1136)\t0.2050195944220032\n",
      "  (4177, 908)\t0.190723753907491\n",
      "  (4177, 859)\t0.1180485242630233\n",
      "  (4177, 389)\t0.284937580621009\n",
      "  (4177, 272)\t0.23160361247061007\n",
      "  (4177, 255)\t0.2915690245844227\n",
      "  (4177, 139)\t0.2915690245844227\n",
      "  (4177, 128)\t0.2915690245844227\n",
      "  (4177, 116)\t0.21085161624990184\n",
      "  (4177, 36)\t0.2915690245844227\n",
      "  (4178, 3443)\t0.2733784609094523\n",
      "  (4178, 2196)\t0.5198719742199562\n",
      "  (4178, 2188)\t0.25322227083471055\n",
      "  (4178, 1882)\t0.18736323312004977\n",
      "  (4178, 1731)\t0.3589760394639486\n",
      "  (4178, 907)\t0.41781431900474814\n",
      "  (4178, 879)\t0.5023328039108282\n"
     ]
    }
   ],
   "source": [
    "tfidf_vect.vocabulary_\n",
    "# tfidf_vect.get\n",
    "print(xtrain_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9870782483847811\n"
     ]
    }
   ],
   "source": [
    "# step 4: \n",
    "# Model training\n",
    "# This is the generalized function for training any given model:\n",
    "\n",
    "def train_model(classifier, feature_vector_train, label,feature_vector_valid, is_neural_net=False):\n",
    "\n",
    " # fit the training dataset on the classifier\n",
    " classifier.fit(feature_vector_train, label)\n",
    " # predict the labels on validation dataset\n",
    " predictions = classifier.predict(feature_vector_valid)\n",
    " return metrics.accuracy_score(predictions, valid_y)\n",
    "\n",
    "# Naive Bayes trainig\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(alpha=0.2),xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "#importance of alpha in Naive_bayes technique\n",
    "# alpha is a smooth-ning parameter. \n",
    "# alpha > 1  indicates laplace smooth-ning\n",
    "# alpha < 1  indicates Lidstone smooth-ning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9641062455132807\n"
     ]
    }
   ],
   "source": [
    "# trying one mor classifier, so that we can compare its performance with Naive Bayes\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(),xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can suppress the above warning :\n",
    "https://machinelearningmastery.com/how-to-fix-futurewarning-messages-in-scikit-learn/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Naive Bayes is giving better results than the linear classifier. We can try\n",
    "many more classifiers and then choose the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommended Reading \n",
    "https://www.datacamp.com/community/tutorials/understanding-logistic-regression-python\n",
    "\n",
    "Naive Bayes is giving better results than the linear classifier. We can try many more classifiers and then choose the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carrying Out Sentiment Analysis\n",
    "--\n",
    "In this section, we are going to discuss how to understand the sentiment of\n",
    "a particular sentence or statement. Sentiment analysis is one of the widely\n",
    "used techniques across the industries to understand the sentiments of the\n",
    "customers/users around the products/services. Sentiment analysis gives\n",
    "the sentiment score of a sentence/statement tending toward positive or negative.\n",
    "\n",
    "Problem\n",
    "--\n",
    "You want to do a sentiment analysis.\n",
    "\n",
    "Solution\n",
    "--\n",
    "The simplest way to do this by using a TextBlob or vedar library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How It Works\n",
    "--\n",
    "Let’s follow the steps in this section to do sentiment analysis using TextBlob. \n",
    "It will basically give 2 metrics.\n",
    "\n",
    "• Polarity = Polarity lies in the range of [-1,1] where 1 means a positive statement and -1 means a negative statement.\n",
    "\n",
    "• Subjectivity = Subjectivity refers that mostly it is a public opinion and not factual information [0,1], i.e 0 means public opinion and 1 means factual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like this phone screen quality and camera clarity is really good\n",
      "This tv is not good Bad quality no clarity worst experience\n"
     ]
    }
   ],
   "source": [
    "# Create the sample data\n",
    "\n",
    "review = \"I like this phone. screen quality and camera clarity is really good.\"\n",
    "review2 = \"This tv is not good. Bad quality, no clarity, worst experience\"\n",
    "\n",
    "# Cleaning and preprocessing\n",
    "def processRow(row):\n",
    " import re\n",
    " import nltk\n",
    " from textblob import TextBlob\n",
    " from nltk.corpus import stopwords\n",
    " from nltk.stem import PorterStemmer\n",
    " from textblob import Word\n",
    " from nltk.util import ngrams\n",
    " import re\n",
    " from nltk.tokenize import word_tokenize\n",
    " tweet = row\n",
    "\n",
    "#Lower case\n",
    " tweet.lower()\n",
    "\n",
    "#Removes unicode strings like \"\\u002c\"  -> ,(comma)\n",
    " tweet = re.sub(r'(\\\\u[0-9A-Fa-f]+)',r'', tweet)\n",
    "    \n",
    "# Removes non-ascii characters. note : \\x00 to \\x7f is 00 to 255\n",
    "# non-ascii characters like copyrigth symbol, trademark symbol\n",
    " tweet = re.sub(r'[^\\x00-\\x7f]',r'',tweet)\n",
    "               \n",
    "#convert any url to URL\n",
    " tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "               \n",
    "#Convert any @Username to \"AT_USER\"\n",
    " tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n",
    "\n",
    "#Remove additional white spaces\n",
    " tweet = re.sub('[\\s]+', ' ', tweet)\n",
    " tweet = re.sub('[\\n]+', ' ', tweet)\n",
    "\n",
    "#Remove not alphanumeric symbols white spaces\n",
    " tweet = re.sub(r'[^\\w]', ' ', tweet)\n",
    "\n",
    "#Removes hastag in front of a word \"\"\"\n",
    " tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "\n",
    "#Replace #word with word\n",
    " tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "\n",
    "#Remove :( or :)\n",
    " tweet = tweet.replace(':)','')\n",
    " tweet = tweet.replace(':(','')\n",
    "\n",
    "#remove numbers\n",
    " tweet = ''.join([i for i in tweet if not i.isdigit()])\n",
    "\n",
    "#remove multiple exclamation\n",
    " tweet = re.sub(r\"(\\!)\\1+\", ' ', tweet)\n",
    "\n",
    "#remove multiple question marks\n",
    " tweet = re.sub(r\"(\\?)\\1+\", ' ', tweet)\n",
    "\n",
    "#remove multistop\n",
    " tweet = re.sub(r\"(\\.)\\1+\", ' ', tweet)\n",
    "\n",
    "#lemma\n",
    " from textblob import Word\n",
    " tweet =\" \".join([Word(word).lemmatize() for word in tweet.split()])\n",
    "\n",
    "#stemmer\n",
    "#st = PorterStemmer()\n",
    "#tweet=\" \".join([st.stem(word) for word in tweet.split()])\n",
    "#Removes emoticons from text\n",
    " tweet = re.sub(':\\)|;\\)|:-\\)|\\(-:|:-D|=D|:P|xD|X-p|\\^\\^|:-*|\\^\\.\\^|\\^\\-\\^|\\^\\_\\^|\\,-\\)|\\)-:|:\\'\\(|:\\(|:-\\(|:\\S|T\\.T|\\.\\_\\.|:<|:-\\S|:-<|\\*\\-\\*|:O|=O|=\\-O|O\\.o|XO|O\\_O|:-\\@|=/|:/|X\\-\\(|>\\.<|>=\\(|D:', '', tweet)\n",
    "\n",
    "#trim\n",
    " tweet = tweet.strip('\\'\"')\n",
    "               \n",
    " row = tweet\n",
    " return row\n",
    "               \n",
    "#call the function with your data\n",
    "review = processRow(review)\n",
    "review2 = processRow(review2)\n",
    "print(review)\n",
    "print(review2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.7, subjectivity=0.6000000000000001)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the sentiment scores\n",
    "\n",
    "# import libraries\n",
    "from textblob import TextBlob\n",
    "\n",
    "#TextBlob has a pre trained sentiment prediction model\n",
    "blob = TextBlob(review)\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=-0.6833333333333332, subjectivity=0.7555555555555555)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Again using TextBlob, over review2\n",
    "blob = TextBlob(review2)\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This is a negative review, as the polarity is “-0.68.”\n",
    "\n",
    "Note: We will cover a one real-time use case on sentiment analysis with\n",
    "an end-to-end implementation in the next chapter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
