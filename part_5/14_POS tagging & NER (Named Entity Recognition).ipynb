{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tagging Part of Speech\n",
    "--\n",
    "Part of speech (POS) tagging is another crucial part of natural language\n",
    "processing that involves labeling the words with a part of speech such as\n",
    "noun, verb, adjective, etc. POS is the base for Named Entity Resolution,\n",
    "Sentiment Analysis, Question Answering, and Word Sense Disambiguation.\n",
    "\n",
    "Problem\n",
    "--\n",
    "Tagging the parts of speech for a sentence.\n",
    "\n",
    "Solution\n",
    "--\n",
    "There are 2 ways a tagger can be built.\n",
    "\n",
    "• Rule based - Rules created manually, which tag a word belonging to a particular POS.\n",
    "\n",
    "• Stochastic based - These algorithms capture the sequence of the words and tag the probability of the sequence using hidden Markov models.\n",
    "\n",
    "Again, NLTK has the best POS tagging module. nltk.pos_tag(word) is the\n",
    "function that will generate the POS tagging for any given word. Use for loop\n",
    "and generate POS for all the words present in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('love', 'VBP'), ('NLP', 'NNP'), ('I', 'PRP'), ('learn', 'VBP'), ('NLP', 'RB'), ('2', 'CD'), ('month', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "text = \"I love NLP and I will learn NLP in 2 month\"\n",
    "\n",
    "# NLTK for POS\n",
    "# Importing necessary packages and stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = sent_tokenize(text)\n",
    "\n",
    "#Generate tagging for all the tokens using loop\n",
    "for i in tokens:\n",
    " words = nltk.word_tokenize(i)\n",
    " words = [w for w in words if not w in stop_words]\n",
    "\n",
    "# POS-tagger.\n",
    "tags = nltk.pos_tag(words)\n",
    "\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Below are the short forms and explanation of POS tagging. \n",
    "for example : The word “love” is VBP, which means verb; sing , dance.\n",
    "\n",
    "• CC coordinating conjunction\n",
    "• CD cardinal digit\n",
    "• DT determiner\n",
    "• EX existential there (like: “there is” ... think of it like “there exists”)\n",
    "• FW foreign word\n",
    "• IN preposition/subordinating conjunction\n",
    "• JJ adjective ‘big’\n",
    "• JJR adjective, comparative ‘bigger’\n",
    "• JJS adjective, superlative ‘biggest’\n",
    "• LS list marker 1)  2)\n",
    "• MD modal could, will\n",
    "• NN noun, singular ‘desk’\n",
    "• NNS noun plural ‘desks’\n",
    "• NNP proper noun, singular ‘Harrison’\n",
    "• NNPS proper noun, plural ‘Americans’\n",
    "• PDT predeterminer ‘all the kids’\n",
    "• POS possessive ending parent’s\n",
    "• PRP personal pronoun I, he, she\n",
    "• PRP$ possessive pronoun my, his, hers\n",
    "• RB adverb very, silently\n",
    "• RBR adverb, comparative better\n",
    "• RBS adverb, superlative best\n",
    "• RP particle give up\n",
    "• TO to go ‘to’ the store\n",
    "• UH interjection\n",
    "• VB verb, base form take\n",
    "• VBD verb, past tense took\n",
    "• VBG verb, gerund/present participle taking\n",
    "• VBN verb, past participle taken\n",
    "• VBP verb, sing. present, non-3d take\n",
    "• VBZ verb, 3rd person sing. present takes\n",
    "• WDT wh-determiner which\n",
    "• WP wh-pronoun who, what\n",
    "• WP$ possessive wh-pronoun whose\n",
    "• WRB wh-adverb where, when"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Entities from Text\n",
    "--\n",
    "In this coding example, we are going to discuss how to identify and extract entities from the text, called Named Entity Recognition. \n",
    "\n",
    "There are multiple libraries to perform this task like NLTK chunker, StanfordNER, SpaCy, opennlp, and NeuroNER; and there are a lot of APIs also like WatsonNLU, AlchemyAPI, NERD, Google Cloud NLP API, and many more.\n",
    "\n",
    "Problem\n",
    "--\n",
    "You want to identify and extract entities from the text.\n",
    "\n",
    "Solution\n",
    "--\n",
    "The simplest way to do this is by using the ne_chunk from NLTK or SpaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The Ghostscript executable isn't found.\n",
      "See http://web.mit.edu/ghostscript/www/Install.htm\n",
      "If you're using a Mac, you can try installing\n",
      "https://docs.brew.sh/Installation then `brew install ghostscript`\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\nltk\\tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    805\u001b[0m                             \u001b[0menv_vars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PATH'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 806\u001b[1;33m                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    807\u001b[0m                         )\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_binary\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    696\u001b[0m         find_binary_iter(\n\u001b[1;32m--> 697\u001b[1;33m             \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_to_bin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    698\u001b[0m         )\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_binary_iter\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    680\u001b[0m     for file in find_file_iter(\n\u001b[1;32m--> 681\u001b[1;33m         \u001b[0mpath_to_bin\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    682\u001b[0m     ):\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_file_iter\u001b[1;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[0;32m    638\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'='\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m75\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 639\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the gs file!\nUse software specific configuration paramaters or set the PATH environment variable.\n===========================================================================",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\nltk\\tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    817\u001b[0m                                         \"https://docs.brew.sh/Installation then `brew install ghostscript`\")                \n\u001b[0;32m    818\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre_error_message\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tree('S', [Tree('PERSON', [('John', 'NNP')]), ('is', 'VBZ'), ('studying', 'VBG'), ('at', 'IN'), Tree('ORGANIZATION', [('Stanford', 'NNP'), ('University', 'NNP')]), ('in', 'IN'), Tree('GPE', [('California', 'NNP')])])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"John is studying at Stanford University in California\"\n",
    "\n",
    "#import libraries\n",
    "import nltk\n",
    "\n",
    "from nltk import ne_chunk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "#NER\n",
    "ne_chunk(nltk.pos_tag(word_tokenize(sent)), binary=False)\n",
    "\n",
    "# second binary parameter indicates whether Named entity is Person, \n",
    "# Org or GPE.\n",
    "# i.e binary=False would help you in classifying the NE"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Here \"John\" is tagged as \"PERSON\"\n",
    "\"Stanford\" as \"ORGANIZATION\"\n",
    "\"California\" as \"GPE\". Geopolitical entity, i.e. countries, cities, states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Cases of NER Models\n",
    "--\n",
    "Named Entity Recognition has a wide range of applications in the field of Natural Language Processing and Information Retrieval. \n",
    "\n",
    "Few such examples have been listed below :\n",
    "\n",
    "1> Automatically Summarizing Resumes : One of the key challenges faced by the HR Department across companies is to evaluate a gigantic pile of resumes to shortlist candidates. To add to their burden, resumes of applicants are often excessively populated in detail, of which, most of the information is irrelevant to what the evaluator is seeking. With the aim of simplifying this process, through our NER model, we could facilitate evaluation of resumes at a quick glance, thereby simplifying the effort required in shortlisting candidates among a pile of resumes.\n",
    "\n",
    "2> Optimizing Search Engine Algorithms : To design a search engine algorithm, instead of searching for an entered query across the millions of articles and websites online, a more efficient approach would be to run an NER model on the articles once and store the entities associated with them permanently. The key tags in the search query can then be compared with the tags associated with the website articles for a quick and efficient search.\n",
    "\n",
    "3> Powering Recommender Systems : NER can be used in developing algorithms for recommender systems which automatically filter relevant content we might be interested in and accordingly guide us to discover related and unvisited relevant contents based on our previous behaviour. This may be achieved by extracting the entities associated with the content in our history or previous activity and comparing them with label assigned to other unseen content to filter relevant ones.\n",
    "\n",
    "4> Simplifying Customer Support : NER can be used in recognizing relevant entities in customer complaints and feedback such as Product specifications, department or company branch details, so that the feedback is classified accordingly and forwarded to the appropriate department responsible for the identified product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion Case study ( 20 - 30 mins )\n",
    "--\n",
    "Automatic Resume Summarization using NER\n",
    "\n",
    "> Each team of 2 participants is expected to scan through this link :\n",
    "https://towardsdatascience.com/a-review-of-named-entity-recognition-ner-using-automatic-summarization-of-resumes-5248a75de175\n",
    "> and be ready for a Q n A round with the trainer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommended Reading - Home Assignment\n",
    "--\n",
    "https://medium.freecodecamp.org/an-introduction-to-part-of-speech-tagging-and-the-hidden-markov-model-953d45338f24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing NER Using SpaCy\n",
    "--\n",
    "Prerequiste : Micrsosoft Visual Studio C++ build tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading https://files.pythonhosted.org/packages/0a/19/2b2c0e1340131a8e23ce4a9804cdccdd62d4d23d3d86c1754857b3de7a14/spacy-2.2.4-cp36-cp36m-win_amd64.whl (9.9MB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading https://files.pythonhosted.org/packages/b0/71/a58322c3489bf0f5a71aa69a66b42164cbc4f0d5ac5e1042c11233766b3f/preshed-3.0.2-cp36-cp36m-win_amd64.whl (105kB)\n",
      "Collecting plac<1.2.0,>=0.9.6\n",
      "  Downloading https://files.pythonhosted.org/packages/86/85/40b8f66c2dd8f4fd9f09d59b22720cffecf1331e788b8a0cab5bafb353d1/plac-1.1.3-py2.py3-none-any.whl\n",
      "Collecting catalogue<1.1.0,>=0.0.7\n",
      "  Downloading https://files.pythonhosted.org/packages/6c/f9/9a5658e2f56932e41eb264941f9a2cb7f3ce41a80cb36b2af6ab78e2f8af/catalogue-1.0.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\program files\\python36\\lib\\site-packages (from spacy) (2.22.0)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading https://files.pythonhosted.org/packages/dd/ec/904b4741879a2a280a40d5bf0b61449a20d1f75281e14ebee06566f7765b/cymem-2.0.3-cp36-cp36m-win_amd64.whl\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\program files\\python36\\lib\\site-packages (from spacy) (1.18.0)\n",
      "Collecting wasabi<1.1.0,>=0.4.0\n",
      "  Downloading https://files.pythonhosted.org/packages/21/e1/e4e7b754e6be3a79c400eb766fb34924a6d278c43bb828f94233e0124a21/wasabi-0.6.0-py3-none-any.whl\n",
      "Collecting tqdm<5.0.0,>=4.38.0\n",
      "  Downloading https://files.pythonhosted.org/packages/4a/1c/6359be64e8301b84160f6f6f7936bbfaaa5e9a4eab6cbc681db07600b949/tqdm-4.45.0-py2.py3-none-any.whl (60kB)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\python36\\lib\\site-packages (from spacy) (42.0.2)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading https://files.pythonhosted.org/packages/15/8f/0dad3ca706e31258cf7b9adf40f8d2103444a09dd7d66d46cf6980025c65/murmurhash-1.0.2-cp36-cp36m-win_amd64.whl\n",
      "Collecting srsly<1.1.0,>=1.0.2\n",
      "  Downloading https://files.pythonhosted.org/packages/3a/d6/939d46c05289b185226a576421079468123b1719ffe16e181e0005d45ef9/srsly-1.0.2-cp36-cp36m-win_amd64.whl (179kB)\n",
      "Collecting blis<0.5.0,>=0.4.0\n",
      "  Downloading https://files.pythonhosted.org/packages/7b/98/00e345edf2ef6d66a8c3cd08779a9829f13c76b37bf3c2445e9965881c2f/blis-0.4.1-cp36-cp36m-win_amd64.whl (5.0MB)\n",
      "Collecting thinc==7.4.0\n",
      "  Downloading https://files.pythonhosted.org/packages/dd/f5/4c76b84a9ae0ea6c659285cf8fed8cce76d5db5b8353e1e08b8d3f56058e/thinc-7.4.0-cp36-cp36m-win_amd64.whl (2.1MB)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\program files\\python36\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.3.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\program files\\python36\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python36\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\program files\\python36\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.7)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\program files\\python36\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\program files\\python36\\lib\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in c:\\program files\\python36\\lib\\site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (8.0.2)\n",
      "Installing collected packages: cymem, murmurhash, preshed, plac, catalogue, wasabi, tqdm, srsly, blis, thinc, spacy\n",
      "Successfully installed blis-0.4.1 catalogue-1.0.0 cymem-2.0.3 murmurhash-1.0.2 plac-1.1.3 preshed-3.0.2 spacy-2.2.4 srsly-1.0.2 thinc-7.4.0 tqdm-4.45.0 wasabi-0.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-c008ece08d5a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pip install spacy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Read/create a sentence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"exists\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Path or Path-like to model data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "# doing NER Using SpaCy\n",
    "!pip install spacy\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# Read/create a sentence\n",
    "doc = nlp(u'Apple is ready to launch new phone worth $10000 in New york time square ')\n",
    "for ent in doc.ents:\n",
    " print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "\n",
    "# for running spaCy, Microsoft Visual Studio C++ build tools error would arise. \n",
    "# I downloaded, added it to the path, .. did lots of Stackoverflow, but still \n",
    "# spaCy on my PC is not able to detect Micrsosoft Visual Studio C++\n",
    "# This error may come up in some Windows based machine , \n",
    "# due to certain missing dlls.\n",
    "\n",
    "# I run the above code on someone's Google colab account and \n",
    "# got the following o/p\n",
    "# Apple 0 5 ORG\n",
    "# 10000 42 47 MONEY\n",
    "# New york 51 59 GPE"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "According to the output, Apple is an organization, 10000 is money, and\n",
    "New York is place. The results are accurate and can be used for any NLP\n",
    "applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional Extra Reading :\n",
    "https://towardsdatascience.com/a-simple-word-sense-disambiguation-application-3ca645c56357\n",
    "\n",
    "You may need to check this stackoverflow link for better understanding :\n",
    "https://towardsdatascience.com/a-simple-word-sense-disambiguation-application-3ca645c56357"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
