{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a Text Preprocessing Pipeline\n",
    "--\n",
    "So far, we have completed most of the text manipulation and processing\n",
    "techniques and methods. In this section, let’s do something interesting.\n",
    "\n",
    "Problem\n",
    "--\n",
    "You want to build an end-to-end text preprocessing pipeline. Whenever\n",
    "you want to do preprocessing for any NLP application, you can directly\n",
    "plug in data to this pipeline function and get the required clean text data as\n",
    "the output.\n",
    "\n",
    "Solution\n",
    "--\n",
    "The simplest way to do this by creating the custom function with all the\n",
    "techniques learned so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read/create the text data\n",
    "# Let’s create a list of strings and assign it to a variable. \n",
    "# Maybe a tweet sample:\n",
    "tweet_sample= \"How to take control of your #debt https://personal.vanguard.com/us/insights/saving-investing/debt-management. #Best advice for #family #financial #success (@PrepareToWin)\"\n",
    "\n",
    "# or you can use any other tweet data extracted in Module-1 , topic-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How to take control of your debt URL Best advice for family financial success AT_USER'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute the below function to process the tweet:\n",
    "\n",
    "def processRow(row):\n",
    " import re\n",
    " import nltk\n",
    " from textblob import TextBlob\n",
    " from nltk.corpus import stopwords\n",
    " from nltk.stem import PorterStemmer\n",
    " from textblob import Word\n",
    " from nltk.util import ngrams\n",
    " import re\n",
    " from nltk.tokenize import word_tokenize\n",
    " tweet = row\n",
    "\n",
    "#Lower case\n",
    " tweet.lower()\n",
    "\n",
    "#Removes unicode strings like \"\\u002c\"  -> ,(comma)\n",
    " tweet = re.sub(r'(\\\\u[0-9A-Fa-f]+)',r'', tweet)\n",
    "    \n",
    "# Removes non-ascii characters. note : \\x00 to \\x7f is 00 to 255\n",
    "# non-ascii characters like copyrigth symbol, trademark symbol\n",
    " tweet = re.sub(r'[^\\x00-\\x7f]',r'',tweet)\n",
    "               \n",
    "#convert any url to URL\n",
    " tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    \n",
    "#     (www\\.[^\\s]+)|(https?://[^\\s]+)\n",
    "\n",
    "#     (www\\.[^\\s]+)\n",
    "#     www.google.com   --> URL\n",
    "\n",
    "#     (https?://[^\\s]+)\n",
    "#     https://google.com  --> URL\n",
    "#     http://google.com   --> URL\n",
    "               \n",
    "#Convert any @Username to \"AT_USER\"\n",
    " tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n",
    "    \n",
    "\n",
    "#Remove additional white spaces\n",
    " tweet = re.sub('[\\s]+', ' ', tweet)\n",
    " tweet = re.sub('[\\n]+', ' ', tweet)\n",
    "\n",
    "#Remove not alphanumeric symbols white spaces\n",
    " tweet = re.sub(r'[^\\w]', ' ', tweet)\n",
    "\n",
    "#Removes hastag in front of a word \"\"\"\n",
    " tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "\n",
    "#Replace #word with word\n",
    " tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "\n",
    "#Remove :( or :)\n",
    " tweet = tweet.replace(':)','')\n",
    " tweet = tweet.replace(':(','')\n",
    "\n",
    "#remove numbers\n",
    " tweet = ''.join([i for i in tweet if not i.isdigit()])\n",
    "\n",
    "#remove multiple exclamation\n",
    " tweet = re.sub(r\"(\\!)\\1+\", ' ', tweet)\n",
    "\n",
    "#remove multiple question marks\n",
    " tweet = re.sub(r\"(\\?)\\1+\", ' ', tweet)\n",
    "\n",
    "#remove multistop\n",
    " tweet = re.sub(r\"(\\.)\\1+\", ' ', tweet)\n",
    "\n",
    "#lemma\n",
    " from textblob import Word\n",
    " tweet =\" \".join([Word(word).lemmatize() for word in tweet.split()])\n",
    "\n",
    "#stemmer\n",
    "#st = PorterStemmer()\n",
    "#tweet=\" \".join([st.stem(word) for word in tweet.split()])\n",
    "#Removes emoticons from text\n",
    " tweet = re.sub(':\\)|;\\)|:-\\)|\\(-:|:-D|=D|:P|xD|X-p|\\^\\^|:-*|\\^\\.\\^|\\^\\-\\^|\\^\\_\\^|\\,-\\)|\\)-:|:\\'\\(|:\\(|:-\\(|:\\S|T\\.T|\\.\\_\\.|:<|:-\\S|:-<|\\*\\-\\*|:O|=O|=\\-O|O\\.o|XO|O\\_O|:-\\@|=/|:/|X\\-\\(|>\\.<|>=\\(|D:', '', tweet)\n",
    "\n",
    "#trim\n",
    " tweet = tweet.strip('\\'\"')\n",
    "               \n",
    " row = tweet\n",
    " return row\n",
    "               \n",
    "#call the function with your data\n",
    "processRow(tweet_sample)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Note : The above text pre-processing function totally depends on type / nature of data. One has to accordingly code his function. Above is just an example to clean tweet data. Depending on the kind of cleaning you plan on the data the above function can go from simple to complex coding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
