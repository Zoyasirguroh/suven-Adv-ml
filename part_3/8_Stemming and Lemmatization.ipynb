{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and Lemmatization\n",
    "--\n",
    "\n",
    "A> Stemming\n",
    "--\n",
    "In this code example, we will discuss stemming. Stemming is a process of\n",
    "extracting a root word. For example, “fish,” “fishes,” and “fishing” are\n",
    "stemmed into fish.\n",
    "\n",
    "Problem\n",
    "--\n",
    "You want to do stemming.\n",
    "\n",
    "Solution\n",
    "--\n",
    "The simplest way to do this by using NLTK or a TextBlob library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               tweet\n",
      "0                     I like fishing\n",
      "1                         I eat fish\n",
      "2  There are lots of fishes in pound\n"
     ]
    }
   ],
   "source": [
    "# Read the text data\n",
    "text=['I like fishing','I eat fish','There are lots of fishes in pound']\n",
    "\n",
    "#convert list to dataframe\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'tweet':text})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       I like fish\n",
       "1                        I eat fish\n",
       "2    there are lot of fish in pound\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming the text\n",
    "\n",
    "#Import library\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "## use PorterStemmer to find stem of the words \n",
    "st = PorterStemmer()\n",
    "\n",
    "df['tweet'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "## type your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# If you observe this, you will notice that fish, fishing, and fishes \n",
    "# have been stemmed to fish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B> Lemmatizing\n",
    "--\n",
    "In this section, we will discuss lemmatization. \n",
    "\n",
    "Lemmatization is a process of extracting a root word by considering the vocabulary. For example, “good,” “better,” or “best” is lemmatized into good.\n",
    "The part of speech of a word is determined in lemmatization. It will\n",
    "return the dictionary form of a word, which must be a valid word while\n",
    "stemming just extracts the root word.\n",
    "\n",
    "> Lemmatization handles matching “car” to “cars” along with matching “car” to “automobile.”\n",
    "\n",
    "> Stemming handles matching “car” to “cars.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization can get better results.\n",
    "\n",
    "> The stemmed form of leafs is leaf.\n",
    "\n",
    "> The stemmed form of leaves is leav.\n",
    "\n",
    "> The lemmatized form of leafs is leaf.\n",
    "\n",
    "> The lemmatized form of leaves is leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem\n",
    "--\n",
    "You want to perform lemmatization.\n",
    "\n",
    "Solution\n",
    "--\n",
    "The simplest way to do this is by using NLTK or the TextBlob library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            tweet\n",
      "0                  I like fishing\n",
      "1                      I eat fish\n",
      "2  There are many fishes in pound\n",
      "3                 leaves and leaf\n"
     ]
    }
   ],
   "source": [
    "text=['I like fishing','I eat fish','There are many fishes in pound', 'leaves and leaf']\n",
    "\n",
    "#convert list to dataframe\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'tweet':text})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                  I like fishing\n",
       "1                      I eat fish\n",
       "2    There are many fish in pound\n",
       "3                   leaf and leaf\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatizing the data\n",
    "\n",
    "#Import library\n",
    "from textblob import Word\n",
    "\n",
    "#Code for lemmatize\n",
    "## type your code here\n",
    "\n",
    "df['tweet'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]) )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can observe that fish and fishes are lemmatized to fish and, as\n",
    "explained, leaves and leaf are lemmatized to leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Zuhrah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bat\n",
      "are\n",
      "foot\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# Init the Wordnet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize Single Word\n",
    "print(lemmatizer.lemmatize(\"bats\"))\n",
    "#> bat\n",
    "\n",
    "print(lemmatizer.lemmatize(\"are\"))\n",
    "#> are\n",
    "\n",
    "print(lemmatizer.lemmatize(\"feet\"))\n",
    "#> foot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Let’s lemmatize a simple sentence. We first tokenize the sentence into words using nltk.word_tokenize and then we will call lemmatizer.lemmatize() on each word. \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'striped', 'bats', 'are', 'hanging', 'on', 'their', 'feet', 'for', 'best']\n",
      "The striped bat are hanging on their foot for best\n"
     ]
    }
   ],
   "source": [
    "# Define the sentence to be lemmatized\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "\n",
    "# Tokenize: Split the sentence into words\n",
    "word_list = nltk.word_tokenize(sentence)\n",
    "print(word_list)\n",
    "#> ['The', 'striped', 'bats', 'are', 'hanging', 'on', 'their', 'feet', 'for', 'best']\n",
    "\n",
    "# Lemmatize list of words and join\n",
    "lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "print(lemmatized_output)\n",
    "#> The striped bat are hanging on their foot for best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "The above code is a simple example of how to use the wordnet lemmatizer on words and sentences.\n",
    "\n",
    "Notice it didn’t do a good job. Because, ‘are’ is not converted to ‘be’ and ‘hanging’ is not converted to ‘hang’ as expected. This can be corrected if we provide the correct ‘part-of-speech’ tag (POS tag) as the second argument to lemmatize().\n",
    "\n",
    "Sometimes, the same word can have a multiple lemmas based on the meaning / context.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strip\n",
      "stripe\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"stripes\", 'v'))  \n",
    "#> strip\n",
    "\n",
    "print(lemmatizer.lemmatize(\"stripes\", 'n'))  \n",
    "#> stripe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Wordnet Lemmatizer with appropriate POS tag\n",
    "-------------------------------------------\n",
    "\n",
    "It may not be possible manually provide the corrent POS tag for every word for large texts. So, instead, we will find out the correct POS tag for each word, map it to the right input character that the WordnetLemmatizer accepts and pass it as the second argument to lemmatize().\n",
    "\n",
    "So how to get the POS tag for a given word?\n",
    "\n",
    "In nltk, it is available through the nltk.pos_tag() method. It accepts only a list (list of words), even if its a single word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('feet', 'NNS')]\n",
      "[('The', 'DT'), ('striped', 'JJ'), ('bats', 'NNS'), ('are', 'VBP'), ('hanging', 'VBG'), ('on', 'IN'), ('their', 'PRP$'), ('feet', 'NNS'), ('for', 'IN'), ('best', 'JJS')]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.pos_tag(['feet']))\n",
    "#> [('feet', 'NNS')]\n",
    "\n",
    "print(nltk.pos_tag(nltk.word_tokenize(sentence)))\n",
    "#> [('The', 'DT'), ('striped', 'JJ'), ('bats', 'NNS'), ('are', 'VBP'), ('hanging', 'VBG'), ('on', 'IN'), ('their', 'PRP$'), ('feet', 'NNS'), ('for', 'IN'), ('best', 'JJS')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "nltk.pos_tag() returns a tuple with the POS tag.\n",
    "\n",
    "The key here is to map NLTK’s POS tags to the format wordnet lemmatizer would accept.\n",
    "\n",
    "The get_wordnet_pos() function defined below does this mapping job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foot\n",
      "['The', 'strip', 'bat', 'be', 'hang', 'on', 'their', 'foot', 'for', 'best']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize with POS Tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "# [('feet', 'NNS'), ('The', 'DT') ]\n",
    "\n",
    "# [0] -> ('feet', 'NNS')\n",
    "# [0][1] -> 'NNS'\n",
    "# [0][1][0] -> 'N'\n",
    "\n",
    "# if no tag matches , then default is wordnet.NOUN\n",
    "# for e.g ([word])[0][1][0] extracts 'D' from [('The', 'DT')]\n",
    "# but 'D' is not their in tag_dict hence return the default as 'Noun'\n",
    "# 1. Init Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# 2. Lemmatize Single Word with the appropriate POS tag\n",
    "word = 'feet'\n",
    "print(lemmatizer.lemmatize(word, get_wordnet_pos(word)))\n",
    "\n",
    "# 3. Lemmatize a Sentence with the appropriate POS tag\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "print([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)])\n",
    "#> ['The', 'strip', 'bat', 'be', 'hang', 'on', 'their', 'foot', 'for', 'best']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#------------------------------------------------\n",
    "\n",
    "# Extra Reading for Data Science lovers :\n",
    "https://www.datacamp.com/community/tutorials/stemming-lemmatization-python\n",
    "\n",
    "https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "\n",
    "#-------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
