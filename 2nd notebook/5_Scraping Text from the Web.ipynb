{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping Text from the Web\n",
    "----------------------------------------\n",
    "\n",
    "> Caution : Before scraping any websites, blogs, or e-commerce websites, please make sure you read the terms and conditions of the websites on whether it gives permissions for data scraping.\n",
    "\n",
    "Web scraping, is also called web harvesting or web data extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a technique to extract a large amount of data from websites and\n",
    "save it in a database or locally. You can use this data to extract information\n",
    "related to your customers/users/products for the business’s benefit.\n",
    "\n",
    "> Prerequisite: Basic understanding of HTML structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem\n",
    "------------\n",
    "You want to extract data from the web by scraping. Here we have taken the\n",
    "example of the IMDB website for scraping top movies.\n",
    "\n",
    "Solution\n",
    "------------\n",
    "The simplest way to do this is by using beautiful soup or scrapy library\n",
    "from Python. Let’s use beautiful soup in this notebook examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\program files\\python36\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\program files\\python36\\lib\\site-packages (from bs4) (4.8.1)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\program files\\python36\\lib\\site-packages (from beautifulsoup4->bs4) (1.9.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\program files\\python36\\lib\\site-packages (2.22.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\program files\\python36\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\program files\\python36\\lib\\site-packages (from requests) (1.25.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python36\\lib\\site-packages (from requests) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\program files\\python36\\lib\\site-packages (from requests) (2.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# Install all the necessary libraries\n",
    "!pip install bs4\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Understand the website page structure to extract the required information\n",
    "\n",
    "Go to the website and right-click on the page content to inspect the html\n",
    "structure of the website.\n",
    "\n",
    "Identify the data and fields you want to extract. Say, for example, we\n",
    "want to fetch information about the <a> tags from our company home page, then follow the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "# of <a> tags are : 9\n",
      "<class 'bs4.element.Tag'>\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "<img alt=\"\" class=\"bg\" src=\"/jpmjpg/1320746967317.jpg\"/>\n",
      "--------------------\n",
      "{'class': ['bg'], 'src': '/jpmjpg/1320746967317.jpg', 'alt': ''}\n",
      "--------------------\n",
      "<class 'bs4.element.Tag'>\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "<img alt=\"\" class=\"bg\" src=\"/jpmjpg/1320746967317.jpg\"/>\n",
      "--------------------\n",
      "{'class': ['bg'], 'src': '/jpmjpg/1320746967317.jpg', 'alt': ''}\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "#---scraping a web page and finding all <a> tags\n",
    "import requests, bs4\n",
    "\n",
    "res = requests.get('https://www.jpmorgan.com/country/IN/EN/jpmorgan')\n",
    "res.raise_for_status()\n",
    "textDataForParsing = bs4.BeautifulSoup(res.text, features='lxml')  # lxml is specification for the parser\n",
    "\n",
    "elements = textDataForParsing.select('img')\n",
    "print(type(elements))\n",
    "\n",
    "print(\"# of <a> tags are :\",len(elements))\n",
    "\n",
    "for i in range(0,2) :\n",
    " print(type(elements[i]))\n",
    " print(\"--------------------\")\n",
    " print(elements[i].getText())\n",
    " print(\"--------------------\")\n",
    " print(str(elements[0]))\n",
    " print(\"--------------------\")\n",
    " print(elements[0].attrs)\n",
    " print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Recall\n",
    "\n",
    ">> Project : Automate Google Search : Done in Python-Core+Advanced Course, Chapter 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is a high chance that you might encounter an error while executing the above script because of the following reasons:\n",
    "\n",
    "• Your request to the URL has failed, so maybe you need to try again after some time. This is common in web scraping.\n",
    "\n",
    "• Web pages are dynamic. The HTML tags of websites keep changing. Understand the tags and make small changes in the code in accordance with HTML, and\n",
    "you are good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'page_response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-c234829580d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# cities = soup.find_all('div',attrs={'class':'main_price'})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage_response\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'page_response' is not defined"
     ]
    }
   ],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "# import requests, bs4\n",
    "# # page_link='http://Html_file.html'\n",
    "# # page_response = requests.get(page_link,timeout=5)\n",
    "# # page_content = bs4.BeautifulSoup(open(\"C:\\Html_file.html\"), \"html.parser\")  # lxml is specification for the parser\n",
    "\n",
    "# # prices= page_content.find_all(class_='main_price')\n",
    "\n",
    "# # prices=page_content.find_all('div',attrs={'class':'main_price'})\n",
    "# url = \"C:/Html_file.html\"\n",
    "# page = open(url)\n",
    "# soup = BeautifulSoup(page.read())\n",
    "# prices= soup.find_all(class_='main_price')\n",
    "# # cities = soup.find_all('div',attrs={'class':'main_price'})\n",
    "\n",
    "# print(page_response)\n",
    "# print(page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Price:66.86$']\n",
      "['Discount_price: 55.45$']\n",
      "['Price:50.55$']\n",
      "['Discount_price: 45.55$']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests, bs4\n",
    "from IPython.display import HTML\n",
    "import re\n",
    "file=open('Html_file.html','r')\n",
    "#page_link='Html_file.html'\n",
    "# #page_response = requests.get(page_link,timeout=5)\n",
    "# page_content = bs4.BeautifulSoup(file.read(), \"html.parser\")  # lxml is specification for the parser\n",
    "\n",
    "# prices= page_content.find_all(class_='main_price')\n",
    "\n",
    "# prices=page_content.find_all('div',attrs={'class':'main_price'})\n",
    "regex = re.compile(r'Price:.*\\$')\n",
    "regex1 = re.compile(r'Discount_price:.*\\$')\n",
    "\n",
    "for line in file:\n",
    "    words = regex.findall(line)\n",
    "    words1 = regex1.findall(line)\n",
    "    if words != []:\n",
    "        \n",
    "        print(words)\n",
    "    if words1 != []:\n",
    "        \n",
    "        print(words1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Case Study to Self Understand & Explain in Class\n",
    "------------------------------------------------\n",
    "The Metropolitan Transportation Authority is North America's largest transportation network, serving a population of 15.3 million people across a 5,000-square-mile travel area surrounding New York City through Long Island, southeastern New York State, and Connecticut.\n",
    "\n",
    "The MTA network comprises the nation’s largest bus fleet and more subway and commuter rail cars than all other U.S. transit systems combined. The MTA's operating agencies are MTA New York City Transit, MTA Bus, Long Island Rail Road, Metro-North Railroad, and MTA Bridges and Tunnels.\n",
    "\n",
    "Go to page http://web.mta.info/developers/turnstile.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task to be coded :\n",
    "\n",
    "Try downloading the entire set of data files with a for loop. The code below contains the entire set of code for web scraping the NY MTA turnstile data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the URL you want to webscrape from\n",
    "url = 'http://web.mta.info/developers/turnstile.html'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the URL\n",
    "response = requests.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse HTML and save to BeautifulSoup object\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549\n"
     ]
    }
   ],
   "source": [
    "print(len(soup.findAll('a')))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To download the whole data set, let's do a for loop through all a tags\n",
    "# for i in range(36,len(soup.findAll('a'))+1): #'a' tags are for links\n",
    "# avoid looping their are too many data files. may take around 2-3 hrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/nyct/turnstile/turnstile_200307.txt\n",
      "http://web.mta.info/developers/data/nyct/turnstile/turnstile_200307.txt\n"
     ]
    }
   ],
   "source": [
    "one_a_tag = soup.findAll('a')[35]\n",
    "link = one_a_tag['href']\n",
    "print(link)\n",
    "download_url = 'http://web.mta.info/developers/'+ link\n",
    "print(download_url)\n",
    "# urllib.request.urlretrieve(download_url,'./'+link[link.find('/turnstile_')+1:])\n",
    "# time.sleep(1) #pause the code for a sec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "page_response = requests.get(download_url)\n",
    "\n",
    "turnstile= open('turnstile_200307.csv','w')\n",
    "\n",
    "turnstile.write(requests.get(download_url).text)\n",
    "turnstile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
