{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem Definition\n",
    "---\n",
    "I think one of the important things when you start a new machine learning project is Defining your problem. that means you should understand business problem.( Problem Formalization)\n",
    "\n",
    "> We will be predicting whether a question asked on Quora is sincere or not\n",
    "\n",
    "Source : https://www.kaggle.com/mjbahmani/a-data-science-framework-for-quora\n",
    "\n",
    "Data Source : https://www.kaggle.com/c/quora-insincere-questions-classification/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About Quora**\n",
    "\n",
    "Quora is a platform that empowers people to learn from each other. On Quora, people can ask questions and connect with others who contribute unique insights and quality answers. A key challenge is to weed out insincere questions -- those founded upon false premises, or that intend to make a statement rather than look for helpful answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Business View**\n",
    "\n",
    "An existential problem for any major website today is how to handle toxic and divisive content. Quora wants to tackle this problem head-on to keep their platform a place where users can feel safe sharing their knowledge with the world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is a insincere question?**\n",
    "\n",
    "Is defined as a question intended to make a statement rather than look for helpful answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Quora_moderation_warning](\\datasets\\images/Quora_moderation_warning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Set**\n",
    "\n",
    "We use train.csv and test.csv as Input and we should upload a submission.csv as Output.\n",
    "\n",
    "The training set contains the following 3 features (for Supervised Learning)\n",
    "1. qid - unique question identifier\n",
    "2. question_text - Quora question text\n",
    "3. target - a question labeled \"insincere\" has a value of 1, otherwise 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding a solutiuon for solving the above problem**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "#from wordcloud import WordCloud as wc   # not needed\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pylab as pylab\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import get_dummies\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import warnings\n",
    "import sklearn\n",
    "import string\n",
    "import scipy\n",
    "import numpy\n",
    "import nltk\n",
    "import json\n",
    "import sys\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib: 3.1.2\n",
      "sklearn: 0.22.1\n",
      "scipy: 1.4.1\n",
      "seaborn: 0.9.0\n",
      "pandas: 0.25.3\n",
      "numpy: 1.18.0\n",
      "Python: 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "# printing versions of the important packages\n",
    "print('matplotlib: {}'.format(matplotlib.__version__))\n",
    "print('sklearn: {}'.format(sklearn.__version__))\n",
    "print('scipy: {}'.format(scipy.__version__))\n",
    "print('seaborn: {}'.format(sns.__version__))\n",
    "print('pandas: {}'.format(pd.__version__))\n",
    "print('numpy: {}'.format(np.__version__))\n",
    "print('Python: {}'.format(sys.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would first do EDA ( Exploratory Data Analysis ) over Quora Data set :\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Quora_EDA_steps](images/Quora_EDA_steps.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I start Collection Data by reading training and testing datasets \n",
    "# into Pandas DataFrames.\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_large = pd.read_csv('C:\\Program Files\\Python36\\suven\\Adv ML\\datasets\\datasets/QuoratrainSet.csv')\n",
    "# test_large = pd.read_csv('C:\\Program Files\\Python36\\suven\\Adv ML\\datasets\\datasets/Quoratestdata.csv')\n",
    "\n",
    "train = train_large[ :15000]\n",
    "X = train.drop('target', axis=1)  \n",
    "y = train['target']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text\n",
       "0  00002165364db923c7e6  How did Quebec nationalists see their province...\n",
       "1  000032939017120e6e44  Do you have an adopted dog, how would you enco...\n",
       "2  0000412ca6e4628ce2cf  Why does velocity affect time? Does velocity a...\n",
       "3  000042bf85aa498cd78e  How did Otto von Guericke used the Magdeburg h...\n",
       "4  0000455dfa3e01eae3af  Can I convert montra helicon D to a mountain b..."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check top 5 records of training dataset\n",
    "X.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15000 entries, 0 to 14999\n",
      "Data columns (total 2 columns):\n",
      "qid              15000 non-null object\n",
      "question_text    15000 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 234.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Find the type of features in Quora dataset\n",
    "# i.e get a quick statistics\n",
    "\n",
    "\n",
    "print(X.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15000 entries, 0 to 14999\n",
      "Data columns (total 2 columns):\n",
      "qid              15000 non-null object\n",
      "question_text    15000 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 234.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(X.info())  # see carefully the last value is -> None. \n",
    "                    # indicating that there are no \"Null\" values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train: (15000, 2)\n",
      "Shape of test: (15000,)\n"
     ]
    }
   ],
   "source": [
    "# shape for train and test\n",
    "print('Shape of train:',X.shape)\n",
    "print('Shape of test:',y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "qid              0\n",
       "question_text    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many NA elements in every column!!\n",
    "# Good news, it is Zero!\n",
    "# To check out how many null info are on the dataset, we can use isnull().sum().\n",
    "# recall from info() -> we found that it has zero Nulls. \n",
    "\n",
    "X_train.isnull().sum()\n",
    "X_test.isnull().sum()\n",
    "# data is infact clean and ready for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Droping (12000, 2)\n",
      "Before Droping (3000, 2)\n",
      "After Droping (12000, 2)\n",
      "After Droping (3000, 2)\n"
     ]
    }
   ],
   "source": [
    "# in case , their were NA or None values in any row then we would drop the row.\n",
    "\n",
    "# remove rows that have NA's\n",
    "print('Before Droping',X_train.shape)\n",
    "print('Before Droping',X_test.shape)\n",
    "X_train = X_train.dropna()\n",
    "X_test = X_test.dropna()\n",
    "print('After Droping',X_train.shape)\n",
    "print('After Droping',X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum of num_words in train 55\n",
      "min of num_words in train 2\n",
      "maximum of  num_words in test 51\n",
      "min of num_words in train 3\n"
     ]
    }
   ],
   "source": [
    "# Number of words in the text\n",
    "\n",
    "X_train[\"num_words\"] = X_train[\"question_text\"].apply(lambda x: len(str(x).split()))\n",
    "X_test[\"num_words\"] = X_test[\"question_text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "print('maximum of num_words in train',X_train[\"num_words\"].max())\n",
    "print('min of num_words in train',X_train[\"num_words\"].min())\n",
    "print(\"maximum of  num_words in test\",X_test[\"num_words\"].max())\n",
    "print('min of num_words in train',X_test[\"num_words\"].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum of num_unique_words in train 48\n",
      "maximum of num_unique_words in test 42\n"
     ]
    }
   ],
   "source": [
    "# Number of unique words in the text\n",
    "X_train[\"num_unique_words\"] = X_train[\"question_text\"].apply(lambda x: len(set(str(x).split())))\n",
    "X_test[\"num_unique_words\"] = X_test[\"question_text\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "print('maximum of num_unique_words in train',X_train[\"num_unique_words\"].max())\n",
    "\n",
    "print(\"maximum of num_unique_words in test\",X_test[\"num_unique_words\"].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum of num_stopwords in train 30\n",
      "maximum of num_stopwords in test 29\n"
     ]
    }
   ],
   "source": [
    "# Number of stopwords in the text\n",
    "\n",
    "#from nltk.corpus import stopwords\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "X_train[\"num_stopwords\"] = X_train[\"question_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "X_test[\"num_stopwords\"] = X_test[\"question_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "\n",
    "print('maximum of num_stopwords in train',X_train[\"num_stopwords\"].max())\n",
    "print(\"maximum of num_stopwords in test\",X_test[\"num_stopwords\"].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum of num_punctuations in train 39\n",
      "maximum of num_punctuations in test 25\n"
     ]
    }
   ],
   "source": [
    "# Number of punctuations in the text\n",
    "\n",
    "X_train[\"num_punctuations\"] =X_train['question_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "X_test[\"num_punctuations\"] =X_test['question_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "# print(train.head())\n",
    "# print(test.head())\n",
    "print('maximum of num_punctuations in train',X_train[\"num_punctuations\"].max())\n",
    "print(\"maximum of num_punctuations in test\",X_test[\"num_punctuations\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_punctuations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>00033991dd3302d609e2</td>\n",
       "      <td>do web developer refer to w3c standard practice?</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5538</th>\n",
       "      <td>01143604d4dc4b028344</td>\n",
       "      <td>was life in soviet union much better than in a...</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3647</th>\n",
       "      <td>00b62782e246c25e06d9</td>\n",
       "      <td>what are some reasons that people behave in an...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3195</th>\n",
       "      <td>009f5b16e260b6cae6d8</td>\n",
       "      <td>how can i root my samsung galaxy s7 edge?</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1911</th>\n",
       "      <td>005f71079134f4bb0342</td>\n",
       "      <td>if you had a city as your boyfriend/girlfriend...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       qid                                      question_text  \\\n",
       "74    00033991dd3302d609e2   do web developer refer to w3c standard practice?   \n",
       "5538  01143604d4dc4b028344  was life in soviet union much better than in a...   \n",
       "3647  00b62782e246c25e06d9  what are some reasons that people behave in an...   \n",
       "3195  009f5b16e260b6cae6d8          how can i root my samsung galaxy s7 edge?   \n",
       "1911  005f71079134f4bb0342  if you had a city as your boyfriend/girlfriend...   \n",
       "\n",
       "      num_words  num_unique_words  num_stopwords  num_punctuations  \n",
       "74            8                 8              2                 1  \n",
       "5538         10                 9              4                 1  \n",
       "3647         14                14              8                 1  \n",
       "3195          9                 9              4                 1  \n",
       "1911         14                14             10                 3  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 1: Change all the text to lower case. \n",
    "\n",
    "# This is required as python interprets 'quora' and 'QUORA' differently\n",
    "\n",
    "X_train['question_text'] = [entry.lower() for entry in X_train['question_text']]\n",
    "\n",
    "X_test['question_text'] = [entry.lower() for entry in X_test['question_text']]\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more imports for NLP\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_punctuations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6498</th>\n",
       "      <td>01429e53e4883a27a203</td>\n",
       "      <td>[which, top, 3, biblical, fallacies, or, inacc...</td>\n",
       "      <td>23</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9608</th>\n",
       "      <td>01e0f4e3e8320688ffdc</td>\n",
       "      <td>[how, do, software, download, sites, like, sof...</td>\n",
       "      <td>19</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>003f6d2f760c16445cb9</td>\n",
       "      <td>[would, it, be, possible, to, create, a, bette...</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1917</th>\n",
       "      <td>005fbb834ab6617b046e</td>\n",
       "      <td>[what, is, my, zip, code, for, india, ?]</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14958</th>\n",
       "      <td>02f0dbc16acf93a574f5</td>\n",
       "      <td>[where, is, the, book, 'harry, and, the, wrink...</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        qid  \\\n",
       "6498   01429e53e4883a27a203   \n",
       "9608   01e0f4e3e8320688ffdc   \n",
       "1281   003f6d2f760c16445cb9   \n",
       "1917   005fbb834ab6617b046e   \n",
       "14958  02f0dbc16acf93a574f5   \n",
       "\n",
       "                                           question_text  num_words  \\\n",
       "6498   [which, top, 3, biblical, fallacies, or, inacc...         23   \n",
       "9608   [how, do, software, download, sites, like, sof...         19   \n",
       "1281   [would, it, be, possible, to, create, a, bette...         20   \n",
       "1917            [what, is, my, zip, code, for, india, ?]          7   \n",
       "14958  [where, is, the, book, 'harry, and, the, wrink...          9   \n",
       "\n",
       "       num_unique_words  num_stopwords  num_punctuations  \n",
       "6498                 21             10                 1  \n",
       "9608                 16              6                 2  \n",
       "1281                 20              8                 1  \n",
       "1917                  7              4                 1  \n",
       "14958                 8              5                 3  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 2 : Tokenization : In this each entry in the corpus will be broken \n",
    "#                         into set of words\n",
    "\n",
    "\n",
    "X_train['question_text']= [word_tokenize(entry) for entry in X_train['question_text']]\n",
    "\n",
    "X_test['question_text']= [word_tokenize(entry) for entry in X_test['question_text']]\n",
    "\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       qid                                      question_text  \\\n",
      "74    00033991dd3302d609e2  [do, web, developer, refer, to, w3c, standard,...   \n",
      "5538  01143604d4dc4b028344  [was, life, in, soviet, union, much, better, t...   \n",
      "3647  00b62782e246c25e06d9  [what, are, some, reasons, that, people, behav...   \n",
      "3195  009f5b16e260b6cae6d8  [how, can, i, root, my, samsung, galaxy, s7, e...   \n",
      "1911  005f71079134f4bb0342  [if, you, had, a, city, as, your, boyfriend/gi...   \n",
      "\n",
      "      num_words  num_unique_words  num_stopwords  num_punctuations  \\\n",
      "74          8.0               8.0            2.0               1.0   \n",
      "5538       10.0               9.0            4.0               1.0   \n",
      "3647       14.0              14.0            8.0               1.0   \n",
      "3195        9.0               9.0            4.0               1.0   \n",
      "1911       14.0              14.0           10.0               3.0   \n",
      "\n",
      "                                    question_text_final  \n",
      "74    ['suppose', 'write', 'statement', 'ielts', 'de...  \n",
      "5538     ['find', 'machine', 'learn', 'course', 'pune']  \n",
      "3647  ['best', 'bachelor', 'degree', 'offer', 'color...  \n",
      "3195  ['gulenist', 'movement', 'stop', 'falsely', 'a...  \n",
      "1911  ['many', 'job', 'employ', 'form', 'deceit', 'a...  \n"
     ]
    }
   ],
   "source": [
    "# step 3, 4 and 5\n",
    "# Remove Stop words and Numeric data \n",
    "# and perfom Word Stemming/Lemmenting.\n",
    "\n",
    "# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb\n",
    "# or adjective etc. By default it is set to Noun\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "# the tag_map would map any tag to 'N' (Noun) except\n",
    "# Adjective to J, Verb -> v, Adverb -> R\n",
    "# that means if you get a Pronoun then it would still be mapped to Noun\n",
    "\n",
    "\n",
    "for index,entry in enumerate(X_train['question_text']):\n",
    "    # Declaring Empty List to store the words that follow the rules for this step\n",
    "    Final_words = []\n",
    "    \n",
    "    # Initializing WordNetLemmatizer()\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    \n",
    "    # pos_tag function below will provide the 'tag' \n",
    "    # i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    for word, tag in pos_tag(entry):\n",
    "        # Below condition is to check for Stop words and consider only \n",
    "        # alphabets\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "            \n",
    "    # The final processed set of words for each iteration will be stored \n",
    "    # in 'question_text_final'\n",
    "    X_train.loc[index,'question_text_final'] = str(Final_words)  \n",
    "    \n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        qid  \\\n",
      "6498   01429e53e4883a27a203   \n",
      "9608   01e0f4e3e8320688ffdc   \n",
      "1281   003f6d2f760c16445cb9   \n",
      "1917   005fbb834ab6617b046e   \n",
      "14958  02f0dbc16acf93a574f5   \n",
      "\n",
      "                                           question_text  num_words  \\\n",
      "6498   [which, top, 3, biblical, fallacies, or, inacc...       23.0   \n",
      "9608   [how, do, software, download, sites, like, sof...       19.0   \n",
      "1281   [would, it, be, possible, to, create, a, bette...       20.0   \n",
      "1917            [what, is, my, zip, code, for, india, ?]        7.0   \n",
      "14958  [where, is, the, book, 'harry, and, the, wrink...        9.0   \n",
      "\n",
      "       num_unique_words  num_stopwords  num_punctuations  \\\n",
      "6498               21.0           10.0               1.0   \n",
      "9608               16.0            6.0               2.0   \n",
      "1281               20.0            8.0               1.0   \n",
      "1917                7.0            4.0               1.0   \n",
      "14958               8.0            5.0               3.0   \n",
      "\n",
      "                                     question_text_final  \n",
      "6498                                                 NaN  \n",
      "9608                                                 NaN  \n",
      "1281   ['scope', 'canada', 'android', 'development', ...  \n",
      "1917   ['free', 'mmorpgs', 'play', 'mac', 'preferably...  \n",
      "14958                                                NaN  \n"
     ]
    }
   ],
   "source": [
    "# step 3, 4 and 5\n",
    "# Remove Stop words and Numeric data \n",
    "# and perfom Word Stemming/Lemmenting.\n",
    "\n",
    "# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb\n",
    "# or adjective etc. By default it is set to Noun\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "# the tag_map would map any tag to 'N' (Noun) except\n",
    "# Adjective to J, Verb -> v, Adverb -> R\n",
    "# that means if you get a Pronoun then it would still be mapped to Noun\n",
    "\n",
    "\n",
    "for index,entry in enumerate(X_test['question_text']):\n",
    "    # Declaring Empty List to store the words that follow the rules for this step\n",
    "    Final_words_test = []\n",
    "    \n",
    "    # Initializing WordNetLemmatizer()\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    \n",
    "    # pos_tag function below will provide the 'tag' \n",
    "    # i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    for word, tag in pos_tag(entry):\n",
    "        # Below condition is to check for Stop words and consider only \n",
    "        # alphabets\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            Final_words_test.append(word_Final)\n",
    "            \n",
    "    # The final processed set of words for each iteration will be stored \n",
    "    # in 'question_text_final'\n",
    "    X_test.loc[index,'question_text_final'] = str(Final_words_test)    \n",
    "\n",
    "print(X_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-5bb7443fb7e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mTfidf_vect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mTfidf_vect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'question_text_final'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mTrain_X_Tfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidf_vect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'question_text_final'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1834\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1835\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_for_unused_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1836\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1837\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1838\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m-> 1220\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m   1221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1222\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1129\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1131\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1132\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \"\"\"\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "Tfidf_vect = TfidfVectorizer()\n",
    "Tfidf_vect.fit(X_train['question_text_final'])\n",
    "\n",
    "\n",
    "Train_X_Tfidf = Tfidf_vect.transform(X_train['question_text_final'])\n",
    "\n",
    "# Test_X_Tfidf = Tfidf_vect.transform(X_test['question_text_final'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TfidfVectorizer' object has no attribute 'vocabulary_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-5d4db5e31880>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# You can use the below syntax to see the vocabulary that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# it has learned from the corpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTfidf_vect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'TfidfVectorizer' object has no attribute 'vocabulary_'"
     ]
    }
   ],
   "source": [
    "# You can use the below syntax to see the vocabulary that \n",
    "# it has learned from the corpus\n",
    "print(Tfidf_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 11939)\t0.30069751448370113\n",
      "  (0, 10855)\t0.4848705185236342\n",
      "  (0, 10708)\t0.4968836778542565\n",
      "  (0, 9021)\t0.534199959102793\n",
      "  (0, 9018)\t0.3771188134083847\n",
      "  (1, 14862)\t0.18744864502808342\n",
      "  (1, 12147)\t0.3383931780682121\n",
      "  (1, 10005)\t0.19449855559871665\n",
      "  (1, 4393)\t0.3799035394097826\n",
      "  (1, 3935)\t0.3014794820607904\n",
      "  (1, 198)\t0.7598070788195652\n",
      "  (2, 14321)\t0.6816644513132366\n",
      "  (2, 13539)\t0.20030929495367858\n",
      "  (2, 12542)\t0.2773844978302303\n",
      "  (2, 5477)\t0.37839876312335247\n",
      "  (2, 249)\t0.5244825817900651\n",
      "  (3, 14500)\t0.4297809332572122\n",
      "  (3, 14194)\t0.19803570932994952\n",
      "  (3, 9631)\t0.44809094845120234\n",
      "  (3, 8035)\t0.44809094845120234\n",
      "  (3, 6011)\t0.4167897707322595\n",
      "  (3, 5761)\t0.44809094845120234\n",
      "  (4, 13911)\t0.3970738498195995\n",
      "  (4, 8826)\t0.39013726524235975\n",
      "  (4, 8760)\t0.44651000583578443\n",
      "  :\t:\n",
      "  (14995, 13376)\t0.3571061914887556\n",
      "  (14995, 10547)\t0.5614028585252218\n",
      "  (14995, 7601)\t0.30713012393983075\n",
      "  (14995, 6881)\t0.5614028585252218\n",
      "  (14995, 1643)\t0.30873299285549494\n",
      "  (14995, 1384)\t0.22909355069710935\n",
      "  (14996, 13447)\t0.35915048829977403\n",
      "  (14996, 12823)\t0.3339203864743801\n",
      "  (14996, 12152)\t0.3563892121876096\n",
      "  (14996, 7817)\t0.3917890479277628\n",
      "  (14996, 4707)\t0.5067651483995899\n",
      "  (14996, 557)\t0.47136531265943665\n",
      "  (14997, 14595)\t0.33847219731636674\n",
      "  (14997, 13138)\t0.5262754402802725\n",
      "  (14997, 13124)\t0.48951272248321437\n",
      "  (14997, 12967)\t0.4598321613215448\n",
      "  (14997, 2647)\t0.3967397264492983\n",
      "  (14998, 13289)\t0.5622652817235035\n",
      "  (14998, 8313)\t0.5532295078722692\n",
      "  (14998, 5011)\t0.6146501969313898\n",
      "  (14999, 12488)\t0.43322708051447123\n",
      "  (14999, 10105)\t0.4491504420466402\n",
      "  (14999, 8313)\t0.42981736700067513\n",
      "  (14999, 5952)\t0.6088490197512002\n",
      "  (14999, 5497)\t0.23481498949348478\n"
     ]
    }
   ],
   "source": [
    "print(Train_X_Tfidf)\n",
    "\n",
    "# Output: \n",
    "# 1: Row number of ‘Train_X_Tfidf’, 0 is the sentence number\n",
    "# 2: Unique Integer number of each word, 2982 is where the word is in the corpus\n",
    "# 3: Score calculated by TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 14817)\t0.246423013630397\n",
      "  (0, 14619)\t0.40015255554868456\n",
      "  (0, 11619)\t0.38176567573815745\n",
      "  (0, 10429)\t0.2868424453874653\n",
      "  (0, 8181)\t0.22460278249406615\n",
      "  (0, 7823)\t0.33921737570028404\n",
      "  (0, 5497)\t0.17354084561856511\n",
      "  (0, 1506)\t0.3412331575750045\n",
      "  (0, 1299)\t0.22976716938140146\n",
      "  (0, 825)\t0.43158513419750283\n",
      "  (1, 14539)\t0.264860351980696\n",
      "  (1, 13045)\t0.27485958772416885\n",
      "  (1, 11389)\t0.49704798960652974\n",
      "  (1, 4429)\t0.4081235424725203\n",
      "  (1, 2707)\t0.2967087287651224\n",
      "  (1, 2666)\t0.3969771825747993\n",
      "  (1, 713)\t0.44164773052978473\n",
      "  (2, 11051)\t0.4994411873004137\n",
      "  (2, 9355)\t0.7825191291141893\n",
      "  (2, 7752)\t0.37178261524488176\n",
      "  (3, 4474)\t1.0\n",
      "  (4, 11051)\t0.40283577553718425\n",
      "  (4, 10005)\t0.29484496439614455\n",
      "  (4, 9316)\t0.582583977298527\n",
      "  (4, 8085)\t0.30956499115747393\n",
      "  :\t:\n",
      "  (2996, 8085)\t0.18309714418289233\n",
      "  (2996, 7594)\t0.3202153218227155\n",
      "  (2996, 7198)\t0.3489456740828173\n",
      "  (2996, 3388)\t0.41128838375225674\n",
      "  (2996, 2043)\t0.3538276792320532\n",
      "  (2996, 1246)\t0.3733089239236176\n",
      "  (2997, 13180)\t0.32318554184296755\n",
      "  (2997, 12542)\t0.36317460765941423\n",
      "  (2997, 10678)\t0.5108731655079867\n",
      "  (2997, 10398)\t0.4582236263656497\n",
      "  (2997, 1758)\t0.4954307951998107\n",
      "  (2997, 1384)\t0.217355363556881\n",
      "  (2998, 12747)\t0.49705214585828733\n",
      "  (2998, 11194)\t0.4794659205164623\n",
      "  (2998, 9864)\t0.46896058273991625\n",
      "  (2998, 5471)\t0.5505702200421047\n",
      "  (2999, 12709)\t0.31872912953404653\n",
      "  (2999, 10426)\t0.4077432995335514\n",
      "  (2999, 10009)\t0.31067454731069777\n",
      "  (2999, 7832)\t0.3304673355533115\n",
      "  (2999, 6519)\t0.37191811123015167\n",
      "  (2999, 6049)\t0.26173299879644446\n",
      "  (2999, 6006)\t0.2571189419920121\n",
      "  (2999, 4779)\t0.38302065314619116\n",
      "  (2999, 3945)\t0.3267837117573953\n"
     ]
    }
   ],
   "source": [
    "print(Test_X_Tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Pre-processing is over !!\n",
    "---\n",
    "\n",
    "Use ML Algorithms to Predict the outcome\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# fit the training dataset on the NB classifier\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "train_Y = train[\"target\"]\n",
    "\n",
    "Naive.fit(Train_X_Tfidf,train_Y)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_NB = Naive.predict(Test_X_Tfidf)\n",
    "\n",
    "print(predictions_NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 3000})"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(predictions_NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({0: 2946, 1: 54})"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "\n",
    "SVM.fit(Train_X_Tfidf,train['target'])\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "\n",
    "print(predictions_SVM)\n",
    "Counter(predictions_SVM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
